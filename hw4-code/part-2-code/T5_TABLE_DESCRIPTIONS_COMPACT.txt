T5 Fine-Tuning Model: Table Descriptions (Compact Version)
===========================================================

DATA PROCESSING:
No preprocessing is applied to raw data. Natural language queries and SQL statements are loaded from .nl and .sql files, paired line-by-line after stripping whitespace. Training set: 4,226 examples; development set: 467 examples; test set: 432 examples. Raw text is fed directly to T5 tokenizer without normalization or cleaning, as T5's SentencePiece tokenizer handles raw text, punctuation, and special characters.

TOKENIZATION:
T5TokenizerFast from google-t5/t5-small with SentencePiece subword tokenization (vocab size: 32,128). Natural language inputs use default T5 tokens (EOS appended). SQL outputs use \texttt{<extra\_id\_0>} (ID: 32099) as BOS token. Dynamic padding via PyTorch's \texttt{pad\_sequence} pads each batch to max length with padding ID 0; attention masks distinguish real vs. padding tokens. Teacher forcing: decoder input = [BOS] + SQL tokens; targets = SQL tokens (one-position offset).

ARCHITECTURE:
T5-small (60M parameters) from google-t5/t5-small checkpoint. 6-layer encoder and 6-layer decoder with 512 hidden dims and 8 attention heads each. Shared 512-dim embeddings; relative positional encoding. Full fine-tuning of all parameters: embeddings, encoder/decoder layers, and LM head. Initialized from pretrained weights (C4 dataset) to adapt to SQL syntax and domain-specific patterns.

HYPERPARAMETERS:
AdamW optimizer: learning rate $1 \times 10^{-4}$ to $5 \times 10^{-4}$, $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10^{-8}$. Weight decay (0.0--0.01) on non-bias/non-LayerNorm params. Batch size: 16; epochs: 10--30. LR scheduling: cosine/linear with 1--3 epoch warmup. Early stopping: patience 3--5 epochs based on validation F1. Loss: cross-entropy on non-padding tokens, averaged per token. Inference: beam search (width 4, max length 512, early stopping).
